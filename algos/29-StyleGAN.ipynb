{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 84\u001B[0m\n\u001B[1;32m     81\u001B[0m     y_fake \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((batch_size, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;66;03m# 更新判别器\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m     d_loss_real \u001B[38;5;241m=\u001B[39m \u001B[43mdiscriminator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_real\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_real\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     d_loss_fake \u001B[38;5;241m=\u001B[39m discriminator\u001B[38;5;241m.\u001B[39mtrain_on_batch(X_fake, y_fake)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# 训练生成器\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:549\u001B[0m, in \u001B[0;36mTensorFlowTrainer.train_on_batch\u001B[0;34m(self, x, y, sample_weight, class_weight, return_dict)\u001B[0m\n\u001B[1;32m    546\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdata\u001B[39m():\n\u001B[1;32m    547\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (x, y, sample_weight)\n\u001B[0;32m--> 549\u001B[0m logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    550\u001B[0m logs \u001B[38;5;241m=\u001B[39m tree\u001B[38;5;241m.\u001B[39mmap_structure(\u001B[38;5;28;01mlambda\u001B[39;00m x: np\u001B[38;5;241m.\u001B[39marray(x), logs)\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:121\u001B[0m, in \u001B[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001B[39;00m\n\u001B[1;32m    120\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(iterator)\n\u001B[0;32m--> 121\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mone_step_on_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m outputs \u001B[38;5;241m=\u001B[39m reduce_per_replica(\n\u001B[1;32m    125\u001B[0m     outputs,\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy,\n\u001B[1;32m    127\u001B[0m     reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    128\u001B[0m )\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:108\u001B[0m, in \u001B[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;129m@tf\u001B[39m\u001B[38;5;241m.\u001B[39mautograph\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mdo_not_convert\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mone_step_on_data\u001B[39m(data):\n\u001B[1;32m    107\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:61\u001B[0m, in \u001B[0;36mTensorFlowTrainer.train_step\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     53\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m(x)\n\u001B[1;32m     54\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute_loss(\n\u001B[1;32m     55\u001B[0m     x\u001B[38;5;241m=\u001B[39mx,\n\u001B[1;32m     56\u001B[0m     y\u001B[38;5;241m=\u001B[39my,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     59\u001B[0m     training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     60\u001B[0m )\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loss_tracker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_state\u001B[49m(\n\u001B[1;32m     62\u001B[0m     loss, sample_weight\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mshape(tree\u001B[38;5;241m.\u001B[39mflatten(x)[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     63\u001B[0m )\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     65\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mscale_loss(loss)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 超参数设置\n",
    "latent_dim = 128\n",
    "img_shape = (64, 64, 3)  # 生成图像的形状\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# 生成器模型\n",
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(4 * 4 * 512, input_dim=latent_dim))\n",
    "    model.add(layers.Reshape((4, 4, 512)))\n",
    "\n",
    "    # 使用转置卷积层逐步放大到64x64\n",
    "    model.add(layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same'))  # 8x8\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))  # 16x16\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))  # 32x32\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2DTranspose(32, kernel_size=4, strides=2, padding='same'))  # 64x64\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(3, kernel_size=7, padding='same', activation='tanh'))  # 输出图像\n",
    "    return model\n",
    "\n",
    "\n",
    "# 判别器模型\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, kernel_size=5, strides=2, padding='same', input_shape=img_shape))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2D(128, kernel_size=5, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Conv2D(256, kernel_size=5, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # 输出概率\n",
    "    return model\n",
    "\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# 编译判别器\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "\n",
    "# 构建生成对抗网络\n",
    "z = layers.Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(img)\n",
    "\n",
    "# 创建组合模型\n",
    "combined = tf.keras.Model(z, validity)\n",
    "combined.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "# 生成数据\n",
    "def generate_latent_points(n):\n",
    "    return np.random.normal(0, 1, (n, latent_dim))\n",
    "\n",
    "\n",
    "# 训练 StyleGAN\n",
    "for epoch in range(epochs):\n",
    "    # 训练判别器\n",
    "    for _ in range(2):  # 判别器训练多次\n",
    "        # 生成真实和假样本\n",
    "        X_real = np.random.uniform(-1, 1, (batch_size, *img_shape))  # 随机生成真实样本\n",
    "        z = generate_latent_points(batch_size)\n",
    "        X_fake = generator.predict(z)\n",
    "\n",
    "        # 标签\n",
    "        y_real = np.ones((batch_size, 1))\n",
    "        y_fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        # 更新判别器\n",
    "        d_loss_real = discriminator.train_on_batch(X_real, y_real)\n",
    "        d_loss_fake = discriminator.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "    # 训练生成器\n",
    "    z = generate_latent_points(batch_size)\n",
    "    g_loss = combined.train_on_batch(z, y_real)  # 目标是让生成器生成的图像被判别器判断为真实的\n",
    "\n",
    "    # 每1000轮打印一次损失\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch: {epoch}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}')\n",
    "\n",
    "\n",
    "# 可视化生成的样本\n",
    "def plot_generated_samples(generator, n=100):\n",
    "    z = generate_latent_points(n)\n",
    "    samples = generator.predict(z)\n",
    "    samples = 0.5 * samples + 0.5  # 归一化到 [0, 1]\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(10, 10, i + 1)\n",
    "        plt.imshow(samples[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_generated_samples(generator)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T14:35:26.101881Z",
     "start_time": "2024-10-14T14:35:25.733054Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
