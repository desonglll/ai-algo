{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m32/32\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 105ms/step - loss: 15.9006\n",
      "Epoch 2/5\n",
      "\u001B[1m32/32\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 110ms/step - loss: 12.7330\n",
      "Epoch 3/5\n",
      "\u001B[1m32/32\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 111ms/step - loss: 14.2599\n",
      "Epoch 4/5\n",
      "\u001B[1m32/32\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 112ms/step - loss: 14.3239\n",
      "Epoch 5/5\n",
      "\u001B[1m32/32\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 113ms/step - loss: 14.3817\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 107ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 32ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "Generated Sequence: [1, 3095, 3095, 3095, 3095, 3095, 3095, 3095, 3095, 3095, 3095]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# 定义位置编码\n",
    "def get_positional_encoding(max_length, d_model):\n",
    "    positions = np.arange(max_length)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe = np.zeros((max_length, d_model))\n",
    "    pe[:, 0::2] = np.sin(positions * div_term)\n",
    "    pe[:, 1::2] = np.cos(positions * div_term)\n",
    "    return tf.constant(pe, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# 定义多头自注意力层\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = layers.Dense(d_model)  # Query\n",
    "        self.wk = layers.Dense(d_model)  # Key\n",
    "        self.wv = layers.Dense(d_model)  # Value\n",
    "\n",
    "        self.dense = layers.Dense(d_model)  # Output layer\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # 将最后一维分成多个头部\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_length, depth)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        q = self.split_heads(self.wq(inputs))  # Query\n",
    "        k = self.split_heads(self.wk(inputs))  # Key\n",
    "        v = self.split_heads(self.wv(inputs))  # Value\n",
    "\n",
    "        # 计算注意力分数\n",
    "        scaled_attention_logits = tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        # 应用注意力权重\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_length, num_heads, depth)\n",
    "\n",
    "        # 连接所有头\n",
    "        output = tf.reshape(output, (tf.shape(output)[0], -1, self.d_model))  # (batch_size, seq_length, d_model)\n",
    "        return self.dense(output)\n",
    "\n",
    "\n",
    "# 定义GPT模型\n",
    "class GPT(keras.Model):\n",
    "    def __init__(self, vocab_size, max_length, d_model, num_heads, num_layers, dropout_rate):\n",
    "        super(GPT, self).__init__()\n",
    "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
    "        self.positional_encoding = get_positional_encoding(max_length, d_model)\n",
    "        self.attention_layers = [MultiHeadSelfAttention(d_model, num_heads) for _ in range(num_layers)]\n",
    "        self.dense = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs) + self.positional_encoding[:tf.shape(inputs)[1], :]\n",
    "        for attention_layer in self.attention_layers:\n",
    "            x = attention_layer(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "# 设置超参数\n",
    "vocab_size = 5000  # 词汇表大小\n",
    "max_length = 50  # 最大序列长度\n",
    "d_model = 128  # 嵌入维度\n",
    "num_heads = 4  # 注意力头数\n",
    "num_layers = 4  # Transformer层数\n",
    "dropout_rate = 0.1  # dropout比例\n",
    "\n",
    "# 构建模型\n",
    "gpt_model = GPT(vocab_size, max_length, d_model, num_heads, num_layers, dropout_rate)\n",
    "\n",
    "# 编译模型\n",
    "gpt_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# 示例数据\n",
    "# 假设有一个训练数据集，输入是形状为(batch_size, max_length)的整数序列\n",
    "# 这里生成一些随机数据作为示例\n",
    "x_train = np.random.randint(0, vocab_size, (1000, max_length))  # 1000个样本\n",
    "y_train = np.random.randint(0, vocab_size, (1000, max_length))  # 目标数据\n",
    "\n",
    "# 训练模型\n",
    "gpt_model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "\n",
    "# 示例：生成文本\n",
    "def generate_text(model, start_token, max_length, num_tokens):\n",
    "    generated = [start_token]\n",
    "    for _ in range(num_tokens):\n",
    "        input_sequence = np.array(generated)[np.newaxis, :]\n",
    "        predictions = model.predict(input_sequence)\n",
    "        next_token = np.argmax(predictions[0, -1, :])  # 获取最后一个token的预测\n",
    "        generated.append(next_token)\n",
    "    return generated\n",
    "\n",
    "\n",
    "# 生成文本示例\n",
    "start_token = 1  # 假设1是开始token\n",
    "generated_sequence = generate_text(gpt_model, start_token, max_length, num_tokens=10)\n",
    "print(\"Generated Sequence:\", generated_sequence)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-14T14:44:42.116179Z",
     "start_time": "2024-10-14T14:44:21.954625Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
