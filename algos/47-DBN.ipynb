{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM层 1 训练完成，第 1 轮\n",
      "RBM层 1 训练完成，第 2 轮\n",
      "RBM层 1 训练完成，第 3 轮\n",
      "RBM层 1 训练完成，第 4 轮\n",
      "RBM层 1 训练完成，第 5 轮\n",
      "RBM层 2 训练完成，第 1 轮\n",
      "RBM层 2 训练完成，第 2 轮\n",
      "RBM层 2 训练完成，第 3 轮\n",
      "RBM层 2 训练完成，第 4 轮\n",
      "RBM层 2 训练完成，第 5 轮\n",
      "微调完成，第 1 轮，损失：2.3055920600891113\n",
      "微调完成，第 2 轮，损失：2.2768776416778564\n",
      "微调完成，第 3 轮，损失：2.327281951904297\n",
      "微调完成，第 4 轮，损失：2.279536247253418\n",
      "微调完成，第 5 轮，损失：2.2749054431915283\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# 定义受限玻尔兹曼机（RBM）模块\n",
    "class RBM(nn.Module):\n",
    "    def __init__(self, visible_units, hidden_units):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(hidden_units, visible_units) * 0.1)\n",
    "        self.h_bias = nn.Parameter(torch.zeros(hidden_units))\n",
    "        self.v_bias = nn.Parameter(torch.zeros(visible_units))\n",
    "\n",
    "    def forward(self, v):\n",
    "        h_prob = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
    "        h_sample = torch.bernoulli(h_prob)\n",
    "        return h_sample\n",
    "\n",
    "    def backward(self, h):\n",
    "        v_prob = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
    "        v_sample = torch.bernoulli(v_prob)\n",
    "        return v_sample\n",
    "\n",
    "    def contrastive_divergence(self, v, k=1):\n",
    "        v0 = v\n",
    "        for _ in range(k):\n",
    "            h = self.forward(v0)\n",
    "            v0 = self.backward(h)\n",
    "        h0 = self.forward(v)\n",
    "        hk = self.forward(v0)\n",
    "\n",
    "        positive_grad = torch.matmul(h0.t(), v)\n",
    "        negative_grad = torch.matmul(hk.t(), v0)\n",
    "\n",
    "        self.W.grad = (positive_grad - negative_grad) / v.size(0)\n",
    "        self.h_bias.grad = torch.mean(h0 - hk, dim=0)\n",
    "        self.v_bias.grad = torch.mean(v - v0, dim=0)\n",
    "\n",
    "\n",
    "# 定义深度信念网络（DBN）\n",
    "class DBN(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbm_layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            rbm = RBM(layer_sizes[i], layer_sizes[i + 1])\n",
    "            self.rbm_layers.append(rbm)\n",
    "        self.classifier = nn.Linear(layer_sizes[-1], 10)  # 假设有10个分类\n",
    "\n",
    "    def pretrain(self, train_loader, epochs=5, lr=0.1):\n",
    "        for idx, rbm in enumerate(self.rbm_layers):\n",
    "            optimizer = optim.SGD(rbm.parameters(), lr=lr)\n",
    "            for epoch in range(epochs):\n",
    "                for data, _ in train_loader:\n",
    "                    data = data.view(data.size(0), -1)\n",
    "                    # 通过前面的RBM获取输入\n",
    "                    for prev_rbm in self.rbm_layers[:idx]:\n",
    "                        data = prev_rbm.forward(data)\n",
    "                    optimizer.zero_grad()\n",
    "                    rbm.contrastive_divergence(data)\n",
    "                    optimizer.step()\n",
    "                print(f\"RBM层 {idx + 1} 训练完成，第 {epoch + 1} 轮\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        for rbm in self.rbm_layers:\n",
    "            x = rbm.forward(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 初始化DBN\n",
    "dbn = DBN([784, 500, 200])\n",
    "\n",
    "# 预训练\n",
    "dbn.pretrain(train_loader)\n",
    "\n",
    "# 微调\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(dbn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = dbn(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"微调完成，第 {epoch + 1} 轮，损失：{loss.item()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T03:10:23.733778Z",
     "start_time": "2024-10-15T03:09:43.559750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
