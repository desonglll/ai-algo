{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前Gym版本: 0.26.2\n",
      "回合 100/2000 - 平均奖励: 0.00 - 探索率: 0.61\n",
      "回合 200/2000 - 平均奖励: 0.01 - 探索率: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeshinoda/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合 300/2000 - 平均奖励: 0.01 - 探索率: 0.22\n",
      "回合 400/2000 - 平均奖励: 0.76 - 探索率: 0.13\n",
      "回合 500/2000 - 平均奖励: 0.88 - 探索率: 0.08\n",
      "回合 600/2000 - 平均奖励: 0.95 - 探索率: 0.05\n",
      "回合 700/2000 - 平均奖励: 0.97 - 探索率: 0.03\n",
      "回合 800/2000 - 平均奖励: 1.00 - 探索率: 0.02\n",
      "回合 900/2000 - 平均奖励: 0.98 - 探索率: 0.01\n",
      "回合 1000/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "回合 1100/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "回合 1200/2000 - 平均奖励: 1.00 - 探索率: 0.01\n",
      "回合 1300/2000 - 平均奖励: 1.00 - 探索率: 0.01\n",
      "回合 1400/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "回合 1500/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "回合 1600/2000 - 平均奖励: 0.98 - 探索率: 0.01\n",
      "回合 1700/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "回合 1800/2000 - 平均奖励: 1.00 - 探索率: 0.01\n",
      "回合 1900/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "回合 2000/2000 - 平均奖励: 0.99 - 探索率: 0.01\n",
      "\n",
      "训练完成！\n",
      "\n",
      "在 100 个测试回合中，成功次数: 100\n",
      "成功率: 100.0%\n",
      "\n",
      "最终的Q表:\n",
      "[[0.73508445 0.77378094 0.6983373  0.73509183]\n",
      " [0.73509189 0.         0.         0.44689223]\n",
      " [0.44689223 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.77377896 0.81450625 0.         0.73509189]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9025     0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.81450625 0.         0.857375   0.77378093]\n",
      " [0.81450625 0.9021613  0.9025     0.        ]\n",
      " [0.857375   0.95       0.         0.85737483]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.95       0.        ]\n",
      " [0.9025     0.94999951 1.         0.9025    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 检查Gym版本\n",
    "import gym\n",
    "from packaging import version\n",
    "\n",
    "gym_version = gym.__version__\n",
    "print(f\"当前Gym版本: {gym_version}\")\n",
    "\n",
    "# 初始化FrozenLake环境\n",
    "# 根据Gym版本选择是否使用gym.make('FrozenLake-v1')或其他方式\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)  # is_slippery=False 时环境是确定性的\n",
    "\n",
    "# 获取状态和动作的数量\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# 初始化Q表为零\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "# 超参数\n",
    "alpha = 0.8  # 学习率\n",
    "gamma = 0.95  # 折扣因子\n",
    "epsilon = 1.0  # 探索率初始值\n",
    "epsilon_min = 0.01  # 探索率最小值\n",
    "epsilon_decay = 0.995  # 探索率衰减率\n",
    "num_episodes = 2000  # 训练回合数\n",
    "max_steps = 100  # 每回合最大步数\n",
    "\n",
    "# 用于记录每个回合的奖励\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # 重置环境并获取初始状态\n",
    "    reset_result = env.reset()\n",
    "    if version.parse(gym_version) >= version.parse(\"0.26\"):\n",
    "        state, _ = reset_result\n",
    "    else:\n",
    "        state = reset_result\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # 决策：使用ε-贪婪策略选择动作\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # 随机探索\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state, :])  # 利用已学知识\n",
    "\n",
    "        # 执行动作，获得下一个状态、奖励和是否完成\n",
    "        step_result = env.step(action)\n",
    "        if version.parse(gym_version) >= version.parse(\"0.26\"):\n",
    "            next_state, reward, terminated, truncated, _ = step_result\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_state, reward, done, _ = step_result\n",
    "\n",
    "        # 更新Q表\n",
    "        old_value = Q_table[state, action]\n",
    "        next_max = np.max(Q_table[next_state, :])\n",
    "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        Q_table[state, action] = new_value\n",
    "\n",
    "        # 累积奖励\n",
    "        total_reward += reward\n",
    "\n",
    "        # 转移到下一个状态\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # 探索率衰减\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    # 每100回合打印一次进度\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = sum(rewards[-100:]) / 100\n",
    "        print(f\"回合 {episode + 1}/{num_episodes} - 平均奖励: {avg_reward:.2f} - 探索率: {epsilon:.2f}\")\n",
    "\n",
    "print(\"\\n训练完成！\\n\")\n",
    "\n",
    "# 测试智能体\n",
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    reset_result = env.reset()\n",
    "    if version.parse(gym_version) >= version.parse(\"0.26\"):\n",
    "        state, _ = reset_result\n",
    "    else:\n",
    "        state = reset_result\n",
    "\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(Q_table[state, :])  # 选择最优动作\n",
    "        step_result = env.step(action)\n",
    "        if version.parse(gym_version) >= version.parse(\"0.26\"):\n",
    "            next_state, reward, terminated, truncated, _ = step_result\n",
    "            done = terminated or truncated\n",
    "        else:\n",
    "            next_state, reward, done, _ = step_result\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            successes += reward\n",
    "            break\n",
    "\n",
    "print(f\"在 {num_test_episodes} 个测试回合中，成功次数: {int(successes)}\")\n",
    "print(f\"成功率: {successes / num_test_episodes * 100}%\")\n",
    "\n",
    "# 打印最终的Q表\n",
    "print(\"\\n最终的Q表:\")\n",
    "print(Q_table)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T03:42:35.615402Z",
     "start_time": "2024-10-15T03:42:34.582831Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
