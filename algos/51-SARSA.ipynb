{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeshinoda/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 93\u001B[0m\n\u001B[1;32m     89\u001B[0m     env\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 93\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 83\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     79\u001B[0m random\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# Gym 的种子现在在 reset 时设置，因此不需要在这里设置\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# 运行 SARSA 算法\u001B[39;00m\n\u001B[0;32m---> 83\u001B[0m Q \u001B[38;5;241m=\u001B[39m \u001B[43msarsa\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.99\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# 评估学习到的策略\u001B[39;00m\n\u001B[1;32m     86\u001B[0m evaluate_policy(env, Q, num_episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "Cell \u001B[0;32mIn[1], line 31\u001B[0m, in \u001B[0;36msarsa\u001B[0;34m(env, num_episodes, alpha, gamma, epsilon)\u001B[0m\n\u001B[1;32m     28\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m---> 31\u001B[0m     next_state, reward, done, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m done \u001B[38;5;129;01mor\u001B[39;00m truncated:\n\u001B[1;32m     34\u001B[0m         \u001B[38;5;66;03m# 回合结束，只使用即时奖励更新 Q 值\u001B[39;00m\n\u001B[1;32m     35\u001B[0m         Q[state, action] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m alpha \u001B[38;5;241m*\u001B[39m (reward \u001B[38;5;241m-\u001B[39m Q[state, action])\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/wrappers/time_limit.py:50\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     40\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:37\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/wrappers/env_checker.py:39\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/envs/toy_text/taxi.py:256\u001B[0m, in \u001B[0;36mTaxiEnv.step\u001B[0;34m(self, a)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, a):\n\u001B[1;32m    255\u001B[0m     transitions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mP[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ms][a]\n\u001B[0;32m--> 256\u001B[0m     i \u001B[38;5;241m=\u001B[39m \u001B[43mcategorical_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtransitions\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnp_random\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m     p, s, r, t \u001B[38;5;241m=\u001B[39m transitions[i]\n\u001B[1;32m    258\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39ms \u001B[38;5;241m=\u001B[39m s\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/gym/envs/toy_text/utils.py:7\u001B[0m, in \u001B[0;36mcategorical_sample\u001B[0;34m(prob_n, np_random)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Sample from categorical distribution where each row specifies class probabilities.\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m prob_n \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(prob_n)\n\u001B[0;32m----> 7\u001B[0m csprob_n \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcumsum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprob_n\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39margmax(csprob_n \u001B[38;5;241m>\u001B[39m np_random\u001B[38;5;241m.\u001B[39mrandom())\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:2586\u001B[0m, in \u001B[0;36mcumsum\u001B[0;34m(a, axis, dtype, out)\u001B[0m\n\u001B[1;32m   2512\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_cumsum_dispatcher)\n\u001B[1;32m   2513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcumsum\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   2514\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2515\u001B[0m \u001B[38;5;124;03m    Return the cumulative sum of the elements along a given axis.\u001B[39;00m\n\u001B[1;32m   2516\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2584\u001B[0m \n\u001B[1;32m   2585\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2586\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcumsum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ai-algo-ozOH1ZQc-py3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:53\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[0;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m         result \u001B[38;5;241m=\u001B[39m wrap(result)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrapfunc\u001B[39m(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     54\u001B[0m     bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, method, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m bound \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def choose_action(state, Q, epsilon, action_space):\n",
    "    \"\"\"\n",
    "    使用 ε-贪婪策略选择动作。\n",
    "    \"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return action_space.sample()  # 探索\n",
    "    else:\n",
    "        return np.argmax(Q[state, :])  # 利用\n",
    "\n",
    "\n",
    "def sarsa(env, num_episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    SARSA 算法实现。\n",
    "    \"\"\"\n",
    "    # 初始化 Q 表\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # 重置环境并设置种子\n",
    "        state, info = env.reset(seed=episode)  # 为每个回合设置不同的种子以确保多样性\n",
    "        action = choose_action(state, Q, epsilon, env.action_space)\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "            if done or truncated:\n",
    "                # 回合结束，只使用即时奖励更新 Q 值\n",
    "                Q[state, action] += alpha * (reward - Q[state, action])\n",
    "            else:\n",
    "                # 选择下一个动作\n",
    "                next_action = choose_action(next_state, Q, epsilon, env.action_space)\n",
    "                # 更新 Q 值\n",
    "                Q[state, action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
    "                # 更新 state 和 action\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # 每100个回合输出一次信息\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"回合数: {episode + 1}, 步数: {step}\")\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def evaluate_policy(env, Q, num_episodes=100):\n",
    "    \"\"\"\n",
    "    使用学习到的 Q 表评估策略。\n",
    "    \"\"\"\n",
    "    total_rewards = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset(seed=episode)  # 为评估设置种子\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state, :])\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "            if done or truncated:\n",
    "                break\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    print(f\"平均奖励: {average_reward}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 创建环境\n",
    "    env = gym.make('Taxi-v3')\n",
    "\n",
    "    # 设置随机种子以确保结果可重复\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    # Gym 的种子现在在 reset 时设置，因此不需要在这里设置\n",
    "\n",
    "    # 运行 SARSA 算法\n",
    "    Q = sarsa(env, num_episodes=10000, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
    "\n",
    "    # 评估学习到的策略\n",
    "    evaluate_policy(env, Q, num_episodes=100)\n",
    "\n",
    "    # 关闭环境\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-15T14:41:42.840750Z",
     "start_time": "2024-10-15T14:40:43.004582Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
